import { useState, useRef, useEffect } from 'react';
import { Mic, ArrowRight, SkipForward } from 'lucide-react';
import { CapturedData } from '../App';

interface MetaPromptViewProps {
  capturedData: CapturedData;
  onConfirm: (dataWithPrompt: CapturedData) => void;
  onBack: () => void;
}

export function MetaPromptView({ capturedData, onConfirm, onBack }: MetaPromptViewProps) {
  const [prompt, setPrompt] = useState('');
  const [isListening, setIsListening] = useState(false);
  const [interimText, setInterimText] = useState(''); // å®æ—¶æ˜¾ç¤ºè¯†åˆ«ä¸­çš„æ–‡å­—
  const [recognitionStatus, setRecognitionStatus] = useState(''); // è¯†åˆ«çŠ¶æ€
  const [browserWarning, setBrowserWarning] = useState(''); // æµè§ˆå™¨å…¼å®¹æ€§è­¦å‘Š
  const [useCloudRecognition, setUseCloudRecognition] = useState(false); // æ˜¯å¦ä½¿ç”¨äº‘ç«¯è¯†åˆ«
  const recognitionRef = useRef<any>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const isPressingRef = useRef(false);
  const isRecognitionActiveRef = useRef(false); // è¯†åˆ«æ˜¯å¦çœŸæ­£åœ¨è¿è¡Œ
  const accumulatedTextRef = useRef(''); // ç´¯ç§¯çš„æ–‡æœ¬
  const textareaRef = useRef<HTMLTextAreaElement>(null);

  // åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«
  useEffect(() => {
    // æ£€æµ‹æµè§ˆå™¨å’Œè¿è¡Œç¯å¢ƒ
    const userAgent = navigator.userAgent.toLowerCase();
    const isIOS = /iphone|ipad|ipod/.test(userAgent);
    const isChrome = /crios/.test(userAgent);
    const isSafari = /safari/.test(userAgent) && !/crios/.test(userAgent) && !/fxios/.test(userAgent);
    
    // æ£€æµ‹æ˜¯å¦åœ¨ PWA æ¨¡å¼ï¼ˆstandalone modeï¼‰
    const isPWA = window.matchMedia('(display-mode: standalone)').matches || 
                  (window.navigator as any).standalone === true;
    
    console.log('ğŸŒ æµè§ˆå™¨ä¿¡æ¯:', { isIOS, isChrome, isSafari, isPWA, userAgent });
    console.log('ğŸ“± iOSç‰ˆæœ¬:', navigator.userAgent.match(/OS (\d+)_/)?.[1]);
    console.log('ğŸ“² è¿è¡Œæ¨¡å¼:', isPWA ? 'PWA (ä¸»å±å¹•å¯åŠ¨)' : 'æµè§ˆå™¨');
    
    // iOS PWA æ¨¡å¼ï¼šä½¿ç”¨äº‘ç«¯å½•éŸ³è¯†åˆ«
    if (isIOS && isPWA) {
      console.log('ğŸŒ æ£€æµ‹åˆ° PWA æ¨¡å¼ï¼Œå°†ä½¿ç”¨äº‘ç«¯è¯­éŸ³è¯†åˆ«');
      setUseCloudRecognition(true);
      return; // ä¸åˆå§‹åŒ– Web Speech API
    }
    
    // iOS Chrome ä¸æ”¯æŒ Web Speech API
    if (isIOS && isChrome) {
      setBrowserWarning('iOS Chrome ä¸æ”¯æŒè¯­éŸ³è¾“å…¥ï¼Œè¯·ä½¿ç”¨ Safari æµè§ˆå™¨æ‰“å¼€');
      console.warn('âš ï¸ iOS Chrome ä¸æ”¯æŒ Web Speech API');
      return;
    }
    
    // æ£€æŸ¥ Web Speech API æ”¯æŒ
    if (!('webkitSpeechRecognition' in window || 'SpeechRecognition' in window)) {
      console.warn('âš ï¸ æµè§ˆå™¨ä¸æ”¯æŒ Web Speech APIï¼Œå°è¯•äº‘ç«¯è¯†åˆ«');
      setUseCloudRecognition(true);
      return;
    }

    // ä½¿ç”¨ Web Speech API (Safari æµè§ˆå™¨)
    const SpeechRecognition = (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition;
    const recognitionInstance = new SpeechRecognition();
    
    // âœ… å…³é”®é…ç½®ï¼šä½¿ç”¨è¿ç»­æ¨¡å¼ï¼Œæ‰‹åŠ¨æ§åˆ¶åœæ­¢
    recognitionInstance.lang = 'zh-CN';
    recognitionInstance.continuous = true; // æ”¹ä¸ºè¿ç»­æ¨¡å¼ï¼Œé¿å…è‡ªåŠ¨åœæ­¢é‡å¯çš„é—®é¢˜
    recognitionInstance.interimResults = true;
    recognitionInstance.maxAlternatives = 1;
    
    console.log('ğŸ¤ è¯†åˆ«å™¨é…ç½®:', {
      lang: recognitionInstance.lang,
      continuous: recognitionInstance.continuous,
      interimResults: recognitionInstance.interimResults
    });

    // onstart: è¯†åˆ«æˆåŠŸå¯åŠ¨
    recognitionInstance.onstart = () => {
      console.log('âœ… [onstart] è¯†åˆ«å·²å¯åŠ¨');
      isRecognitionActiveRef.current = true;
      setRecognitionStatus('éº¦å…‹é£å·²å°±ç»ª');
      accumulatedTextRef.current = '';
      setInterimText('');
    };

    // onaudiostart: å¼€å§‹æ•è·éŸ³é¢‘
    recognitionInstance.onaudiostart = () => {
      console.log('ğŸ™ï¸ [onaudiostart] éŸ³é¢‘æ•è·å¼€å§‹');
      setRecognitionStatus('æ­£åœ¨å¬...');
    };

    // onsoundstart: æ£€æµ‹åˆ°å£°éŸ³
    recognitionInstance.onsoundstart = () => {
      console.log('ğŸ”Š [onsoundstart] æ£€æµ‹åˆ°å£°éŸ³');
    };

    // onspeechstart: æ£€æµ‹åˆ°è¯­éŸ³
    recognitionInstance.onspeechstart = () => {
      console.log('ğŸ—£ï¸ [onspeechstart] æ£€æµ‹åˆ°è¯­éŸ³');
      setRecognitionStatus('æ­£åœ¨è¯†åˆ«...');
    };

    // onresult: è¯†åˆ«åˆ°å†…å®¹
    recognitionInstance.onresult = (event: any) => {
      console.log('ğŸ“¥ [onresult] ç»“æœæ•°é‡:', event.results.length);
      
      let interim = '';
      let final = '';
      
      for (let i = 0; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          final += transcript;
          console.log('  âœ“ æœ€ç»ˆç»“æœ:', transcript);
        } else {
          interim += transcript;
          console.log('  â€¦ ä¸´æ—¶ç»“æœ:', transcript);
        }
      }
      
      // æ˜¾ç¤ºä¸´æ—¶ç»“æœ
      setInterimText(interim);
      
      // ç´¯ç§¯æœ€ç»ˆç»“æœ
      if (final) {
        accumulatedTextRef.current += final;
        console.log('ğŸ“ ç´¯ç§¯æ–‡æœ¬:', accumulatedTextRef.current);
      }
    };

    // onspeechend: è¯­éŸ³ç»“æŸ
    recognitionInstance.onspeechend = () => {
      console.log('ğŸ¤ [onspeechend] è¯­éŸ³ç»“æŸ');
    };

    // onsoundend: å£°éŸ³ç»“æŸ
    recognitionInstance.onsoundend = () => {
      console.log('ğŸ”‡ [onsoundend] å£°éŸ³ç»“æŸ');
    };

    // onaudioend: éŸ³é¢‘æ•è·ç»“æŸ
    recognitionInstance.onaudioend = () => {
      console.log('ğŸ™ï¸ [onaudioend] éŸ³é¢‘æ•è·ç»“æŸ');
    };

    // onerror: é”™è¯¯å¤„ç†
    recognitionInstance.onerror = (event: any) => {
      console.error('âŒ [onerror] é”™è¯¯ç±»å‹:', event.error);
      console.error('   è¯¦ç»†ä¿¡æ¯:', event);
      
      // aborted é”™è¯¯é€šå¸¸æ˜¯å› ä¸ºæ‰‹åŠ¨åœæ­¢æˆ–é‡å¤å¯åŠ¨
      if (event.error === 'aborted') {
        console.log('   â†’ è¯†åˆ«è¢«ä¸­æ­¢ï¼ˆé€šå¸¸æ˜¯æ­£å¸¸åœæ­¢ï¼‰');
        return; // ä¸æ˜¾ç¤ºé”™è¯¯ï¼Œè¿™æ˜¯é¢„æœŸè¡Œä¸º
      }
      
      if (event.error === 'no-speech') {
        setRecognitionStatus('æœªæ£€æµ‹åˆ°è¯­éŸ³');
      } else if (event.error === 'audio-capture') {
        setRecognitionStatus('æ— æ³•è®¿é—®éº¦å…‹é£');
      } else if (event.error === 'not-allowed') {
        setRecognitionStatus('éº¦å…‹é£æƒé™è¢«æ‹’ç»');
        alert('è¯·åœ¨è®¾ç½®ä¸­å…è®¸æ­¤ç½‘ç«™ä½¿ç”¨éº¦å…‹é£');
      } else if (event.error === 'network') {
        setRecognitionStatus('ç½‘ç»œé”™è¯¯');
      } else {
        setRecognitionStatus(`é”™è¯¯: ${event.error}`);
      }
    };
    
    // onend: è¯†åˆ«ç»“æŸ
    recognitionInstance.onend = () => {
      console.log('ğŸ”„ [onend] è¯†åˆ«ç»“æŸ');
      console.log('   â†’ æŒ‰ä½çŠ¶æ€:', isPressingRef.current);
      console.log('   â†’ ç´¯ç§¯æ–‡æœ¬:', accumulatedTextRef.current);
      
      isRecognitionActiveRef.current = false;
      
      // âŒ ä¸å†è‡ªåŠ¨é‡å¯ï¼è®©ç”¨æˆ·é‡æ–°æŒ‰ä¸‹æŒ‰é’®
      setIsListening(false);
      setInterimText('');
      setRecognitionStatus('');
      isPressingRef.current = false;
      
      // ä¿å­˜ç´¯ç§¯çš„æ–‡æœ¬
      if (accumulatedTextRef.current) {
        setPrompt(prev => {
          const newText = prev + accumulatedTextRef.current;
          console.log('ğŸ’¾ ä¿å­˜åˆ°è¾“å…¥æ¡†:', newText);
          return newText;
        });
        accumulatedTextRef.current = '';
      }
    };

    recognitionRef.current = recognitionInstance;
    console.log('ğŸ—ï¸ è¯†åˆ«å™¨åˆå§‹åŒ–å®Œæˆ');

    return () => {
      console.log('ğŸ§¹ æ¸…ç†è¯†åˆ«å™¨');
      if (recognitionRef.current && isRecognitionActiveRef.current) {
        try {
          recognitionRef.current.abort();
        } catch (e) {
          console.warn('æ¸…ç†æ—¶åœæ­¢è¯†åˆ«å¤±è´¥:', e);
        }
      }
    };
  }, []);

  // äº‘ç«¯å½•éŸ³è¯†åˆ« - æŒ‰ä½å¼€å§‹
  const handleCloudRecordStart = async () => {
    console.log('ğŸŒ æŒ‰ä¸‹æŒ‰é’® - ä½¿ç”¨äº‘ç«¯è¯†åˆ«');
    isPressingRef.current = true;
    setIsListening(true);
    setRecognitionStatus('å¯åŠ¨éº¦å…‹é£...');
    audioChunksRef.current = [];

    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const mediaRecorder = new MediaRecorder(stream);
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
          console.log('ğŸ“¼ éŸ³é¢‘æ•°æ®:', event.data.size, 'bytes');
        }
      };
      
      mediaRecorder.onstop = async () => {
        console.log('â¹ï¸ å½•éŸ³åœæ­¢ï¼Œå¼€å§‹è¯†åˆ«...');
        setRecognitionStatus('æ­£åœ¨è¯†åˆ«...');
        
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
        console.log('ğŸ“¦ éŸ³é¢‘æ€»å¤§å°:', audioBlob.size, 'bytes');
        
        // è½¬æ¢ä¸º base64
        const reader = new FileReader();
        reader.onloadend = async () => {
          const base64Audio = (reader.result as string).split(',')[1];
          
          try {
            // è°ƒç”¨åç«¯API
            const { projectId, publicAnonKey } = await import('../utils/supabase/info');
            const response = await fetch(
              `https://${projectId}.supabase.co/functions/v1/make-server-f359b1dc/speech-to-text`,
              {
                method: 'POST',
                headers: {
                  'Content-Type': 'application/json',
                  'Authorization': `Bearer ${publicAnonKey}`
                },
                body: JSON.stringify({ audio: base64Audio })
              }
            );
            
            if (!response.ok) {
              throw new Error(`è¯†åˆ«å¤±è´¥: ${response.status}`);
            }
            
            const { text } = await response.json();
            console.log('âœ… è¯†åˆ«ç»“æœ:', text);
            
            // æ·»åŠ åˆ°è¾“å…¥æ¡†
            setPrompt(prev => prev + text);
            setRecognitionStatus('');
            setIsListening(false);
            
          } catch (error) {
            console.error('âŒ è¯†åˆ«å¤±è´¥:', error);
            setRecognitionStatus('è¯†åˆ«å¤±è´¥');
            setTimeout(() => {
              setRecognitionStatus('');
              setIsListening(false);
            }, 2000);
          }
        };
        reader.readAsDataURL(audioBlob);
        
        // åœæ­¢æ‰€æœ‰éŸ³è½¨
        stream.getTracks().forEach(track => track.stop());
      };
      
      mediaRecorderRef.current = mediaRecorder;
      mediaRecorder.start();
      setRecognitionStatus('æ­£åœ¨å½•éŸ³...');
      console.log('ğŸ¤ å¼€å§‹å½•éŸ³');
      
    } catch (error) {
      console.error('âŒ éº¦å…‹é£è®¿é—®å¤±è´¥:', error);
      setRecognitionStatus('éº¦å…‹é£æƒé™è¢«æ‹’ç»');
      setIsListening(false);
      isPressingRef.current = false;
    }
  };

  // äº‘ç«¯å½•éŸ³è¯†åˆ« - æ¾å¼€åœæ­¢
  const handleCloudRecordEnd = () => {
    console.log('ğŸ‘‹ æ¾å¼€æŒ‰é’® - åœæ­¢å½•éŸ³');
    isPressingRef.current = false;
    
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
    }
  };

  // æŒ‰ä½å¼€å§‹å½•éŸ³
  const handleVoiceStart = () => {
    if (useCloudRecognition) {
      handleCloudRecordStart();
      return;
    }
    
    console.log('ğŸ‘† æŒ‰ä¸‹æŒ‰é’®');
    
    if (!recognitionRef.current) {
      alert('æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¾“å…¥');
      return;
    }

    isPressingRef.current = true;
    setIsListening(true);
    setRecognitionStatus('å¯åŠ¨ä¸­...');

    try {
      recognitionRef.current.start();
      console.log('ğŸ¤ å¼€å§‹è¯†åˆ«');
    } catch (error: any) {
      console.error('å¯åŠ¨å¤±è´¥:', error);
      setRecognitionStatus('å¯åŠ¨å¤±è´¥: ' + error.message);
      if (!error.message?.includes('already')) {
        isPressingRef.current = false;
        setIsListening(false);
      }
    }
  };

  // æ¾å¼€åœæ­¢å½•éŸ³
  const handleVoiceEnd = () => {
    if (useCloudRecognition) {
      handleCloudRecordEnd();
      return;
    }
    
    console.log('ğŸ‘‹ [handleVoiceEnd] æ¾å¼€æŒ‰é’®');
    console.log('   â†’ å½“å‰è¯†åˆ«çŠ¶æ€:', isRecognitionActiveRef.current);
    
    isPressingRef.current = false;
    
    if (recognitionRef.current && isRecognitionActiveRef.current) {
      try {
        console.log('â¹ï¸ è°ƒç”¨ stop()');
        recognitionRef.current.stop();
        // æ³¨æ„ï¼šæ–‡æœ¬ä¿å­˜ä¼šåœ¨ onend äº‹ä»¶ä¸­å¤„ç†ï¼Œä¸åœ¨è¿™é‡Œå¤„ç†
      } catch (error) {
        console.error('âŒ åœæ­¢å¤±è´¥:', error);
      }
    } else {
      console.log('âš ï¸ è¯†åˆ«å™¨æœªè¿è¡Œï¼Œè·³è¿‡åœæ­¢');
      setIsListening(false);
      setRecognitionStatus('');
    }
  };

  const handleConfirm = () => {
    onConfirm({
      ...capturedData,
      userPrompt: prompt.trim() || undefined,
    });
  };

  const handleSkip = () => {
    onConfirm({
      ...capturedData,
      userPrompt: undefined,
    });
  };

  return (
    <div className="h-full bg-black flex flex-col overflow-hidden">
      {/* Header */}
      <div className="flex-shrink-0 px-6 pt-6 pb-3">
        <div className="flex items-center justify-between">
          <button
            onClick={onBack}
            className="w-10 h-10 rounded-full bg-white/10 backdrop-blur-sm flex items-center justify-center active:scale-90 transition-transform"
          >
            <svg className="w-6 h-6" fill="none" viewBox="0 0 24 24">
              <path d="M15 18L9 12L15 6" stroke="white" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"/>
            </svg>
          </button>
          
          <div className="px-4 py-2 rounded-full bg-gradient-to-r from-white to-[#FFFC00] text-black">
            <span className="text-sm font-bold">Meta æ¨¡å¼</span>
          </div>
          
          <div className="w-10" /> {/* Spacer */}
        </div>
      </div>

      {/* Preview Image - ç¼©å°é«˜åº¦ */}
      <div className="flex-shrink-0 px-6 mb-3">
        <div className="rounded-2xl overflow-hidden relative" style={{ height: '35vh', maxHeight: '280px' }}>
          <img 
            src={capturedData.image} 
            alt="Captured" 
            className="w-full h-full object-cover"
          />
          {capturedData.character && (
            <div className="absolute bottom-3 left-3 bg-[#FFFC00] text-black px-3 py-1.5 rounded-full text-xs font-bold">
              è§’è‰²: {capturedData.character.name}
            </div>
          )}
        </div>
      </div>

      {/* Bottom Content Area - å›ºå®šé«˜åº¦ï¼Œå†…éƒ¨å¯æ»šåŠ¨ */}
      <div className="flex-1 flex flex-col px-6 pb-6 min-h-0">
        <div className="mb-3 flex-shrink-0">
          <h2 className="text-white font-bold text-lg mb-1">æè¿°ä½ æƒ³è¦çš„åœºæ™¯</h2>
          <p className="text-white/60 text-xs">
            å‘Šè¯‰ AI ä½ æƒ³è®© {capturedData.character?.name} å‡ºç°åœ¨ä»€ä¹ˆæ ·çš„åœºæ™¯ä¸­
          </p>
        </div>

        {/* Text Input - Fixed Height */}
        <div className="mb-3 h-20 flex-shrink-0">
          <div className="relative h-full">
            <textarea
              ref={textareaRef}
              value={prompt}
              onChange={(e) => setPrompt(e.target.value)}
              placeholder="ä¾‹å¦‚ï¼šåœ¨æµ·è¾¹æ•£æ­¥ï¼Œåœ¨å’–å•¡å…çœ‹ä¹¦ï¼Œåœ¨æ£®æ—é‡Œéœ²è¥..."
              className="w-full h-full bg-white/10 backdrop-blur-sm border-2 border-white/20 rounded-2xl px-4 py-3 text-white placeholder:text-white/40 resize-none focus:outline-none focus:border-[#FFFC00] transition-colors text-sm"
              maxLength={200}
              style={{ WebkitUserSelect: 'text', userSelect: 'text' }}
            />
            {/* å®æ—¶è¯†åˆ«ç»“æœæ˜¾ç¤º */}
            {isListening && interimText && (
              <div className="absolute top-3 left-4 right-4 text-[#FFFC00] text-sm opacity-70 pointer-events-none">
                {interimText}
              </div>
            )}
            <div className="absolute bottom-2 right-3 text-white/40 text-xs">
              {prompt.length}/200
            </div>
          </div>
        </div>

        {/* è¯†åˆ«çŠ¶æ€æŒ‡ç¤ºå™¨ */}
        {isListening && recognitionStatus && (
          <div className="mb-2 flex-shrink-0">
            <div className="flex items-center gap-2 bg-white/5 rounded-lg px-3 py-2">
              <div className="w-2 h-2 bg-[#FFFC00] rounded-full animate-pulse"></div>
              <span className="text-white/80 text-sm">{recognitionStatus}</span>
            </div>
          </div>
        )}

        {/* æµè§ˆå™¨å…¼å®¹æ€§è­¦å‘Š */}
        {browserWarning && (
          <div className="mb-3 flex-shrink-0">
            <div className="bg-orange-500/20 border-2 border-orange-400 rounded-xl px-4 py-3">
              <div className="flex items-start gap-2">
                <div className="text-orange-400 text-lg">âš ï¸</div>
                <div className="flex-1">
                  <div className="text-orange-300 font-bold text-sm mb-1">è¯­éŸ³è¾“å…¥ä¸å¯ç”¨</div>
                  <div className="text-orange-200/90 text-xs mb-2">{browserWarning}</div>
                  <div className="text-white/70 text-xs leading-relaxed">
                    <strong className="text-[#FFFC00]">ğŸ’¡ å¦‚éœ€ä½¿ç”¨è¯­éŸ³ï¼š</strong><br/>
                    1. ç‚¹å‡»å³ä¸Šè§’ "â€¢â€¢â€¢" æˆ– "åˆ†äº«" æŒ‰é’®<br/>
                    2. é€‰æ‹© "åœ¨ Safari ä¸­æ‰“å¼€"<br/>
                    3. è¯­éŸ³åŠŸèƒ½å°†ç«‹å³å¯ç”¨<br/><br/>
                    <span className="text-white/50">æˆ–ç›´æ¥ä½¿ç”¨é”®ç›˜è¾“å…¥åœºæ™¯æè¿°</span>
                  </div>
                </div>
              </div>
            </div>
          </div>
        )}

        {/* Voice Input Button - æŒ‰ä½è¯´è¯ï¼Œç¦ç”¨æ–‡å­—é€‰æ‹© */}
        {!browserWarning && (
          <button
            onMouseDown={handleVoiceStart}
            onMouseUp={handleVoiceEnd}
            onMouseLeave={handleVoiceEnd}
            onTouchStart={(e) => {
              e.preventDefault();
              handleVoiceStart();
            }}
            onTouchEnd={(e) => {
              e.preventDefault();
              handleVoiceEnd();
            }}
            onContextMenu={(e) => e.preventDefault()}
            className={`mb-3 py-3.5 rounded-2xl font-bold flex items-center justify-center gap-2 transition-all flex-shrink-0 ${
              isListening 
                ? 'bg-red-500 text-white animate-pulse scale-95' 
                : 'bg-white/10 backdrop-blur-sm text-white'
            }`}
            style={{ 
              WebkitUserSelect: 'none', 
              userSelect: 'none',
              WebkitTouchCallout: 'none',
              touchAction: 'manipulation'
            }}
          >
            {isListening ? (
              <>
                <div className="w-5 h-5 flex items-center justify-center">
                  <div className="w-3 h-3 bg-white rounded-full animate-ping" />
                </div>
                <span>æ¾å¼€ç»“æŸ...</span>
              </>
            ) : (
              <>
                <Mic className="w-5 h-5" />
                <span>æŒ‰ä½è¯´è¯</span>
              </>
            )}
          </button>
        )}

        {/* Action Buttons */}
        <div className="grid grid-cols-2 gap-3 flex-shrink-0">
          <button
            onClick={handleSkip}
            className="py-3.5 rounded-2xl font-bold bg-white/10 backdrop-blur-sm text-white active:scale-95 transition-transform flex items-center justify-center gap-2"
          >
            <SkipForward className="w-5 h-5" />
            <span>è·³è¿‡</span>
          </button>
          
          <button
            onClick={handleConfirm}
            className="py-3.5 rounded-2xl font-bold bg-gradient-to-r from-white to-[#FFFC00] text-black active:scale-95 transition-transform flex items-center justify-center gap-2"
          >
            <span>ç¡®è®¤</span>
            <ArrowRight className="w-5 h-5" />
          </button>
        </div>
      </div>
    </div>
  );
}